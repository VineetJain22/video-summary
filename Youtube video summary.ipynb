{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Install required packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only once, if the following packages are not installed. \n",
    "!pip install openai\n",
    "!pip install youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required Python libraries\n",
    "import os\n",
    "import openai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Set API & Video Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API key\n",
    "openai.api_key = \"xxx\" # replace xxx with your own Open AI gpt-3.5-turbo API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Key in  the ID of Youtube video that you wish to get a summary of.\n",
    "#For example, in the link https://www.youtube.com/watch?v=CWIOUo5MTZE, ID is CWIOUo5MTZE\n",
    "\n",
    "video_id = \"CWIOUo5MTZE\" \n",
    "transcript = YouTubeTranscriptApi.get_transcript(video_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Get transcript of video\n",
    "Use package youtube_transcript_api to get transcript of the video. This notebook use text format function for formatting the transcript of video. Alternatively, you may also try the json format. As pre-processing, I have removed the new line character from the text.\n",
    "\n",
    "Note that some Youtube videos may not have transcript enabled; this notebook will not work for those videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api.formatters import JSONFormatter\n",
    "from youtube_transcript_api.formatters import TextFormatter\n",
    "\n",
    "formatter = TextFormatter()\n",
    "\n",
    "# .format_transcript(transcript) turns the transcript into a JSON string.\n",
    "text_formatted = formatter.format_transcript(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2270"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_formatted = text_formatted.replace(\"\\n\",\" \")\n",
    "len(text_formatted.split())\n",
    "#text_formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Split processed transcript into chunks\n",
    "The Language Model (LLM) we're using, gpt-3.5-turbo, has a fixed context window size.\n",
    "This means that there's a maximum limit to the number of tokens that can be processed in a single request. If the input text exceeds this maximum limit, the LLM won't be able to handle it and will reject the request.\n",
    "As a result, we need to split any long transcripts into smaller chunks to fit within this context window.\n",
    "\n",
    "Important Note: The context window size must accommodate both the request and the response.\n",
    "In other words, the combined token count of the input text (request) and output text (response)\n",
    "must not exceed the fixed context window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(string, words_per_chunk=3000):\n",
    "    words = string.split()\n",
    "    num_chunks = len(words) // words_per_chunk\n",
    "    if len(words) % words_per_chunk != 0:\n",
    "        num_chunks += 1\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(num_chunks):\n",
    "        start_index = i * words_per_chunk\n",
    "        end_index = start_index + words_per_chunk\n",
    "        chunk = \" \".join(words[start_index:end_index])\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "input_string = text_formatted\n",
    "#input_string = \"This is a sample string containing more than 2500 words. We will split this string into chunks of 2500 words.\"\n",
    "chunked_strings = split_into_chunks(input_string, words_per_chunk=2500)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunked_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Prompt GPT for summary\n",
    "\n",
    "This section of the code deals with sending transcript chunks to the GPT-3.5-turbo API for summarization.\n",
    "Each chunk of the transcript is processed individually. When we send a chunk to the API, it returns a summary of that specific part of the transcript. The API is prompted using a basic structure that includes a 'system' and a 'user' message, which is sufficient for our current needs.\n",
    "\n",
    "Customization Note: The prompt can be tailored to meet specific requirements. For example, you may want to modify it to request the API to return the output in JSON format, or to limit the summary to a certain number of words like 200.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transcript will be split into 1 parts. Summary of each part will be provided.\n",
      "\n",
      "\n",
      "Summary for part 1:\n",
      "- The African Union has been given membership of the G20 under the presidency of Prime Minister Modi.\n",
      "- Sunil Mittal, founder of bti Enterprises and chair of the B20 Africa Wing, has played a significant role in cultivating the India-Africa relationship.\n",
      "- The inclusion of the African Union in the G20 is seen as a major achievement and will be remembered fondly in Africa.\n",
      "- India's historic relationship with Africa has been revitalized, and Indian businesses will be more welcome than ever before in the African continent.\n",
      "- China's approach to Africa has been focused on extractive businesses and has caused indebtedness in African countries.\n",
      "- India, on the other hand, has provided technology support, diplomatic and political support, and soft loans and grants to Africa.\n",
      "- Doing business in Africa has its challenges, such as infrastructure and currency depreciation, but the continent offers great potential for economic development.\n",
      "- The B20 India Action Council focused on promoting African integration and highlighted the need for improved cross-border business within the African Union.\n",
      "- Key areas of focus for Africa's development include agriculture and digital infrastructure.\n",
      "- Africa's growing population and young workforce make it an attractive market for goods and services.\n",
      "- The world is waking up to the potential of Africa, and businesses should start focusing on the continent.\n",
      "- Despite the challenges, Africa is becoming a major growth engine for the global economy.\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"gpt-3.5-turbo\"\n",
    "print(f\"The transcript will be split into {len(chunked_strings)} parts. Summary of each part will be provided.\")\n",
    "summary_parts = \"\"\n",
    "for i, chunk in enumerate(chunked_strings):\n",
    "    #print(f\"Chunk {i+1}:\")\n",
    "    #print(chunk)\n",
    "    #print()\n",
    "    response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Summarize the provided transcript of a youtube video in bullet points. \\\n",
    "        The text of the transcript starts here:\\n\\n{chunk}\"}\n",
    "    ],\n",
    "    temperature=0\n",
    "    )\n",
    "\n",
    "    print(f\"\\n\\nSummary for part {i+1}:\")\n",
    "    print(response['choices'][0]['message']['content'])\n",
    "    summary_parts += response['choices'][0]['message']['content']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
